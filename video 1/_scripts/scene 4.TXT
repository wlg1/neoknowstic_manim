SCENE 4:

So, what does this visual we've just seen have to do with a neural network? The answer actually has been hidden in front of our eyes this entire time. 

Let's look at this equation below, which is used by a neural network. What does our simple unit conversion equation have to do with it? We're going to show that they're very similar.

First, we'll represent the output value as one variable, and not look at units.

Second, note that all the variables in the equation below are matrices. So we'll analogously map multiplication into matrix multiplication by putting our terms in brackets. 

The b is the bias, which we don't need to discuss now. We can show it in our equation above by adding 0 to the left side, noting that adding 0 doesn't change our equation.
    color highlight b in equation

The sigma is called an activation function, which we also don't need to discuss now. For our unit conversion equation, we'll use the Identity function I. We see that when x goes through the identify function I, we get x again. It's like multiplying an expression by 1. So again, this doesn't change our equation. 
    show I(x) = x

Now, both equations look similar. But what is the equation we have below? It's a neuron equation, which is what a neural network uses to compute a neuron activation. So our unit conversion equation is just a very simple neuron. 
    Ïƒ(WX+b) = A appears below

<<<
W contains weight connections that the X inputs are multiplied with, and A contains the neuron activations, which in our case, is the nap neuron. 
    as say each one, copy moves onto NN

In this case, the matrix W actually just consists of a single weight variable lower case w, also called w one. Same for the X  and A matrices.
    Turn W in [w1]

    [dont say this b/c save it for later]: Here, we only have one weight, one input, and one activation, but if we add one more weight and input, we see how this is like matrix multiplication.
        Show matrix multiplication of w in multiple dims

In fact, one can think of this nap neuron as just a single layer neural network with one neuron.

Or with two neurons, if you think of the input as a nose neuron represented by the equation 1*x.
    fade in connection of same input through 1* input to nose neuron. I(1*x + 0) = x

Now, we see that our unit conversion visual has been hiding a neural network neuron all along.

<<<<
So a neural network can be viewed as a function that's composed of many smaller functions. Note that it can't approximate every function, but different types of neural networks can approximate a wide variety of functions that are useful for prediction.
    show eqns composing when adding more layers
    visual of functions w/ linear piecewise (from site)

    move our bold NN into a faded, opaque larger NN w/ hidden layer

Of course, a neural network is more than just this equation, so rather than saying this equation is a neural network, we can say it's analogous to a neural network.
    eqn ~ NN. just move our eqns from above.

But what allows large neural networks to do complex tasks that a simple neural network can't do? Let's see what happens when we add more connections to our network, starting by just adding one more.
    add one connection
