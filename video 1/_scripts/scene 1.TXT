Scene 1: (1m, 30sec)

linger longer for one frame (no music) at start to show cat face clearer
thumbnail: wojaks point at the first frame (colorful vectors too). words: FURRIES + TESSERACTS = ?!?! (diff fonts, persp)
    FURRIES + THE FOURTH \nDIMENSION = ?!?!
    red vector arrow pointing to circle with first frame inside. use both blue and red as 2D axis, red points to it.
background may not be all black; can be a picture

[ Cat + Person = cat person thru NN]
[use cat person mascot when talking w/o anims]
    [music plays first 3 secs]
    starts w/ addition of concepts through neural newtork to form cat at 3 secs, just when Wind in Rushes music 'opens up' with a synth chime and down drops 'Cat Person Mathematics'

[person reading at bottom right, black background. cat sleeping next to purple book, has small zzz bubble. eyes moving, cat sleeping, dreaming of a crown. paper on top left. this paper may be surrounded by a thought bubble from the person]

https://github.com/3b1b/manim/blob/master/manimlib/mobject/svg/drawings.py
    ThoughtBubble

When studying new improvements to neural networks, people often encounter concepts that they're not familiar with, such as manifold, or isometry.
    start w/ scrolling through a paper, blurring it and making concepts from paper appear bigger like news report. OR fade paper to black

    eigendecomposition word appears where paper was.

    fabrege egg
    https://vimeo.com/362452264

So what do these mathematical concepts mean, and why do they use them?
    fade in appear on screen in italics

In this series, we'll be providing answers to these questions in an visual and intuitive way.
    don't go back to paper. replace manifold w/ visual

    [words quickly flash next to obj: * more intuitive than this thing]

[visual in thought bubble replaced by matrix multiplication. person's eyes become half circle but no zzz]

Let's start with a basic concept: using matrix multiplication to recognize features. Neural networks multiply weight matrices with inputs. 
    fade in WX = A

But why does that specific algebraic procedure work? Why does its first step use the dot product of the weight matrix's first row and the input vector to get the first output? And what do dot products have to do with how a neural network recognizes features?
    1.3.py: matrix multp rectangles fade in

    fade in NN

[person gets drowsier, eyes close, then zzz]
[thought bubble now has dream world of latent space. upon 'composes', thought bubble merges with cat's small zzz bubble to have person see cat inside bubble. cat is king, person has jester hat. alice in wonderland checkboard patterns on floor. 

[both watch sky as analogies in NN appear. or person just watches cat king transform (no, too hard to animate). get queen card outline, etc. as eqns. OR blue cat turns to pink cat, still wearing crown. -M (blue) + W (pink) appear next to cat. OR don't use trad colors, use green (M) and purple (W). crown is gold.]

[style editing merges a person into a cat person in the dream]

(40 sec so far; ~25 secs)
Knowing why this procedure is done is key to a visual intuition behind how neural networks compose together patterns into more complex, higher level patterns. For example, they can add concepts together to make analogies, composing together "King minus Man plus Woman to equal Queen" in a hidden, high dimensional world called Latent Space, or perform abstractions such as Style Editing. Throughout this series, we'll be explaining how all these work.
    King – Man + Woman = Queen (not geometrically?)

    https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/

    style editing example

[now cat person watches a 3Blue1Brown video appear in the sky. the 6 videos are given as text or thumbnails, 'video 1', etc. outside thought bubble in dark rectangle bg]

(talk fast, ~20s):
Aside from knowing multiplication, there's no prereqs to watching this. We'll explain a lot of concepts from scratch. If you're unfamiliar with some terms, it's recommended to watch these 6 3Blue1Brown videos, which are in the description. But just like two classes that teach the same material from different perspectives, we'll be reviewing many of those videos' concepts to explain new ones in our series.
    3Blue1Brown’s Linear Algebra playlist, namely videos 1, 2, 3, 9 and 13, and also the first video in his Neural Network playlist
    link to 3b1br, etc. show videos playing at once  and link to them in description.

    https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown

[wind in rushes plays to silence, or fades out to silence for a moment]

[thought bubble expands to entire screen, and 3B video fades out. perhaps expands into cat person's head. fade out.]
Ok, let's begin.

