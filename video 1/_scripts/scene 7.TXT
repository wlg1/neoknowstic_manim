SCENE 7:

We can our equation represent using variables.
    w1*nose + w2*ear = nap

<<<
Again, this has an analogy to a neural network, where w2 is another connection from the ear neuron to the nap neuron.
    transform it to NN with 2 inputs, 1 output

Recall how our equation with only nose to nap could also be represented as a matrix with 1 row and 1 column. Now, we go from a matrix of 1 row and 1 column, to a matrix with 1 row and 2 columns
    w1*nose + (w2) * ear = nap

    one by one, map each to its matrix rep. transform w to w1 when expanding its brackets to the right

    replay this alongside the visual, transforming the steps at same time.

The number of columns in W are the number of inputs, and the number of rows are the number of outputs, meaning there's a connection for every matrix entry.

This explains why we use that strange algebraic procedure of matrix multiplication where we multiply the first row by the first column. This procedure is none other than the Dot Product. 
[explain dot product]

And our dot product is just performing a Change of Units, or in other words, a Change of Basis.
    write out dot product

<<<
length of face

height of person

length of tail, and whether it faces left or right

(not as good b/c 'faces' are like nodes)

<<<<<<<<<<<<
So our Input Space and our Nap Space are measuring neuron activations. We call these spaces Activation Space.

Since the nose and ear neurons in the first layer act as basis vectors in Input Space, and the Nap neuron in the second layer acts as a basis vector in Nap Space, we come to a very important concept:

Neurons are Basis Vectors in an Activation Space.
    write out on screen

Thus, every neuron in a neural network is a measurement on the data, coming up with an interpretation for it from its own perspective through a projection. 
    neuron shadow slice of each output neuron w/ its weights to input only
    2D shadow from 3D NN, 3D neuron spheres? the slice on ground is an output neuron slice.

    wait here for a while to let viewer process

This Activation Space, commonly referred to as a Hidden or Latent Space, is where analogies such as King - Man + Woman = Queen can be made by adding its vectors. 

This is a high dimensional space, where each neuron acts as its dimension, and combinations of neurons can also act as their own dimension.
    latent space animation of adding two vectors to get another
    same as before, but now show 'neuron 1, 2' on axis

    show point on 1D w/ neuron eqn
    the 2D planes show subnetwork on its surnose?


<<
Now that we've connected the geometry of linear algebra to neural networks, we can finally begin to study its world of concepts. What lies in this mysterious realm of AI imagination?

<<<<<
In our examples, for the purpose of gaining intuition, we defined our neurons in terms of “human-understandable measurements”. And though studies show these may exist, such as dog neurons that detect dogs, neurons may not always cleanly correspond to a Human-Defined Concept. 

A network might not even have a 'smile' neuron, even though the pattern of 'smile' is somehow recognized within the neural network. In fact, most of the millions of neurons in many neural networks may not correspond to a human-defined concept. 
    fade in distill paper result w/ citation at bottom that doesn't fade out till end of scene

One alternative possibility is that, instead, each neuron has multiple roles in affecting the calculations of other neurons, similar to if-else branches in a decision tree. But a lot of research still has to be done to find out what circuits of neurons are actually doing.


